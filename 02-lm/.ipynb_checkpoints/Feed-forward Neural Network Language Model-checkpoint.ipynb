{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to realize the model as shown below:\n",
    "\n",
    "\n",
    "![Feed-forward Neural Network Language Model](./img/FNNLM.png)\n",
    "\n",
    "\n",
    "\n",
    "we train the language model and generate sentence by training the model based on PTB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code was inspired by https://github.com/neubig/nn4nlp-code/blob/master/02-lm-pytorch/nn-lm-batch.py\n",
    "# first: import lib\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add all hyper parameters\n",
    "N = 2 # The length of the n-gram\n",
    "EMB_SIZE = 128 # The size of the embedding\n",
    "HID_SIZE = 128 # The size of the hidden layer\n",
    "USE_CUDA = torch.cuda.is_available() # use gpu ?\n",
    "MAX_LEN = 50 # the MAX length of the sentence\n",
    "learning_rate = 0.001 # learning rate\n",
    "drop_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the data can be fed into the model\n",
    "\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<S>\"] # 句子标识\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "def read_dataset(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\")) # train set\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\")) # dev set\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "nwords = len(w2i) # corpus length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "class FNN_LM(nn.Module):\n",
    "    def __init__(self, nwords, emb_size, hid_size, num_hist, dropout):\n",
    "        super(FNN_LM, self).__init__()\n",
    "        self.embedding = nn.Embedding(nwords, emb_size)\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(num_hist*emb_size, hid_size), \n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hid_size, nwords)\n",
    "        )\n",
    "\n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words)      # 3D Tensor of size [batch_size x num_hist x emb_size]\n",
    "        feat = emb.view(emb.size(0), -1) # 2D Tensor of size [batch_size x (num_hist*emb_size)]\n",
    "        logit = self.fnn(feat)           # 2D Tensor of size [batch_size x nwords]\n",
    "        return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and the optimizer\n",
    "model = FNN_LM(nwords=nwords, emb_size=EMB_SIZE, hid_size=HID_SIZE, num_hist=N, dropout=drop_ratio)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful function for train the model\n",
    "\n",
    "# make the data to Variable\n",
    "def to_var(words):\n",
    "    var = Variable(torch.LongTensor(words))\n",
    "    if USE_CUDA:\n",
    "        var = var.cuda()\n",
    "    return var\n",
    "  \n",
    "# A function to calculate scores for one value\n",
    "def calc_score_of_histories(words):\n",
    "    # This will change from a list of histories, to a pytorch Variable whose data type is LongTensor\n",
    "    words_var = to_var(words)\n",
    "    logits = model(words_var)\n",
    "    return logits\n",
    "\n",
    "# Calculate the loss value for the entire sentence\n",
    "def calc_sent_loss(sent):\n",
    "    # The initial history is equal to end of sentence symbols\n",
    "    hist = [S] * N\n",
    "    # Step through the sentence, including the end of sentence token\n",
    "    all_histories = []\n",
    "    all_targets = []\n",
    "    # every time we use the first two word to predict the next word.\n",
    "    for next_word in sent + [S]:\n",
    "        all_histories.append(list(hist))\n",
    "        all_targets.append(next_word)\n",
    "        hist = hist[1:] + [next_word] \n",
    "        logits = calc_score_of_histories(all_histories)\n",
    "        loss = nn.functional.cross_entropy(logits, to_var(all_targets), size_average=False)\n",
    "    return loss\n",
    "\n",
    "# Generate a sentence\n",
    "def generate_sent():\n",
    "    hist = [S] * N\n",
    "    sent = []\n",
    "    while True:\n",
    "        logits = calc_score_of_histories([hist])\n",
    "        prob = nn.functional.softmax(logits)\n",
    "        next_word = prob.multinomial().data[0,0]\n",
    "        if next_word == S or len(sent) == MAX_LEN:\n",
    "            break\n",
    "        sent.append(next_word)\n",
    "        hist = hist[1:] + [next_word]\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--finished 5000 sentences (word/sec=1945.99)\n",
      "--finished 10000 sentences (word/sec=1942.29)\n",
      "--finished 15000 sentences (word/sec=1942.79)\n",
      "--finished 20000 sentences (word/sec=1944.15)\n",
      "--finished 25000 sentences (word/sec=1940.59)\n",
      "--finished 30000 sentences (word/sec=1939.59)\n",
      "--finished 35000 sentences (word/sec=1939.15)\n",
      "--finished 40000 sentences (word/sec=1939.33)\n",
      "iter 0: train loss/word=5.6090, ppl=272.8729 (word/sec=1939.27)\n",
      "iter 0: dev loss/word=5.7387, ppl=310.6734 (word/sec=2804.89)\n",
      "the thrift were used to make it eventually also a him\n",
      "there the had access to plain me could show east six an spending in the british industry officials said in a half\n",
      "should be given the amendment indicated that the oak richard plans\n",
      "and the aimed and <unk> <unk> that sometimes through the current fiscal N 's judgment\n",
      "issued by case this week\n",
      "--finished 5000 sentences (word/sec=1941.61)\n",
      "--finished 10000 sentences (word/sec=1940.87)\n",
      "--finished 15000 sentences (word/sec=1942.52)\n",
      "--finished 20000 sentences (word/sec=1942.77)\n",
      "--finished 25000 sentences (word/sec=1943.18)\n",
      "--finished 30000 sentences (word/sec=1943.97)\n",
      "--finished 35000 sentences (word/sec=1944.39)\n",
      "--finished 40000 sentences (word/sec=1943.58)\n",
      "iter 1: train loss/word=5.5196, ppl=249.5282 (word/sec=1942.85)\n",
      "iter 1: dev loss/word=5.7289, ppl=307.6406 (word/sec=2783.37)\n",
      "it 's hard to tax conditions would help convert any products say i promised a speed a lower economists would be toward investors recalls\n",
      "although other officers of pregnant\n",
      "financial and only seem <unk> to tell them up to $ N million merchant television had turmoil phoenix police might not <unk> shipping <unk> him if the plans to visit to the restrictions will have fourth-quarter results which it will face to brands but use cooperation as or provide an directors reserve to future\n",
      "the insurance interests they are <unk> for goldsmith who competitive own needs to reflect east germans <unk> <unk> in <unk> first growth in the year-ago contracts\n",
      "mr. jones vice president at federal funds are publicly held in a by ask no coming to allowing a model in N\n",
      "--finished 5000 sentences (word/sec=1931.12)\n",
      "--finished 10000 sentences (word/sec=1917.47)\n",
      "--finished 15000 sentences (word/sec=1913.19)\n",
      "--finished 20000 sentences (word/sec=1920.64)\n",
      "--finished 25000 sentences (word/sec=1917.14)\n",
      "--finished 30000 sentences (word/sec=1919.78)\n",
      "--finished 35000 sentences (word/sec=1911.75)\n",
      "--finished 40000 sentences (word/sec=1912.75)\n",
      "iter 2: train loss/word=5.4546, ppl=233.8325 (word/sec=1914.32)\n",
      "iter 2: dev loss/word=5.7149, ppl=303.3667 (word/sec=2799.88)\n",
      "industry from the ual plan oct. N N of having been <unk> it\n",
      "i think they have tremendous sale of its founder or & seize a little conference personnel taxes including debt and imposed their manager to simple <unk> and added it 's a takeover five federal possible shopping upon the advertisers is just as entered higher terms\n",
      "wake in the case where remember its head went through limit of all the man an industry and el paso texas\n",
      "the decline in the york\n",
      "the poll conducted in an los angeles prices that only that when the taxes and takeover facilities and which networks inc. philadelphia with the price of much of home airlines 's tax of <unk>\n",
      "--finished 5000 sentences (word/sec=1933.16)\n",
      "--finished 10000 sentences (word/sec=1934.58)\n"
     ]
    }
   ],
   "source": [
    "# 验证集上的损失\n",
    "last_dev = 1e20\n",
    "\n",
    "# begin to train the model\n",
    "for ITER in range(5):\n",
    "    # Perform training\n",
    "    random.shuffle(train)\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    train_words, train_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    # begin to tran\n",
    "    for sent_id, sent in enumerate(train):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        train_loss += my_loss.data[0]\n",
    "        train_words += len(sent)\n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "        if (sent_id+1) % 5000 == 0:\n",
    "            print(\"--finished %r sentences (word/sec=%.2f)\" % (sent_id+1, train_words/(time.time()-start)))\n",
    "    print(\"iter %r: train loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), train_words/(time.time()-start)))\n",
    "    \n",
    "    # Evaluate on dev set\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    dev_words, dev_loss = 0, 0.0\n",
    "    start = time.time()\n",
    "    for sent_id, sent in enumerate(dev):\n",
    "        my_loss = calc_sent_loss(sent)\n",
    "        dev_loss += my_loss.data[0]\n",
    "        dev_words += len(sent)\n",
    "    \n",
    "    # Keep track of the development accuracy and reduce the learning rate if it got worse\n",
    "    if last_dev < dev_loss:\n",
    "        optimizer.learning_rate /= 2\n",
    "    last_dev = dev_loss\n",
    "    print(\"iter %r: dev loss/word=%.4f, ppl=%.4f (word/sec=%.2f)\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), dev_words/(time.time()-start)))\n",
    "    \n",
    "    # Generate a few sentences\n",
    "    for _ in range(5):\n",
    "        sent = generate_sent()\n",
    "        print(\" \".join([i2w[x] for x in sent]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
