{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Word\n",
    "\n",
    "We want to realize the model as shown below:\n",
    "\n",
    "\n",
    "\n",
    "![Bag Of Word](./img/bow.png)\n",
    "\n",
    "\n",
    "\n",
    "In here, we are doing a text classfication task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a class to contorl all the hyper parameters\n",
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.lr = 1e-3\n",
    "        self.epoch_num = 10\n",
    "        self.train_path = \"../data/classes/train.txt\"\n",
    "        self.test_path = \"../data/classes/test.txt\"\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i)) # we can add the index automatically in this way\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      tag, words = line.lower().strip().split(\" ||| \")\n",
    "      yield ([w2i[x] for x in words.split(\" \")], t2i[tag])\n",
    "# the dataset just like ([id1, id2, id3, ...], tag1)\n",
    "# [id1, id2, id3, ...] present the sentence, tag1 present the semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "train = list(read_dataset(config.train_path))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(config.test_path))\n",
    "nwords = len(w2i)\n",
    "ntags = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "class BOW(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size):\n",
    "        super(BOW, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, tag_size)\n",
    "        self.bow_bias = Parameter(torch.Tensor(tag_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embed(x) # b*w*t   in this case, b = batch = 1\n",
    "        word_score = torch.sum(embeds, 1) # b*w*t -> bxt\n",
    "        scores = word_score.add_(self.bow_bias)\n",
    "        out = F.log_softmax(scores)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "bow = BOW(nwords, ntags)\n",
    "\n",
    "# optim and loss\n",
    "optimizer = optim.Adam(bow.parameters(), lr=config.lr)\n",
    "loss_fn = torch.nn.NLLLoss() # loss(bow(input), target) and last layer of model is LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: train loss/sent=4.4428, time=31.09s\n",
      "iter 0: test acc=0.2222\n",
      "iter 1: train loss/sent=3.3411, time=47.01s\n",
      "iter 1: test acc=0.2480\n",
      "iter 2: train loss/sent=2.6772, time=45.05s\n",
      "iter 2: test acc=0.2656\n",
      "iter 3: train loss/sent=2.2217, time=44.12s\n",
      "iter 3: test acc=0.2805\n",
      "iter 4: train loss/sent=1.8787, time=43.52s\n",
      "iter 4: test acc=0.2955\n",
      "iter 5: train loss/sent=1.6076, time=43.66s\n",
      "iter 5: test acc=0.2982\n",
      "iter 6: train loss/sent=1.3876, time=43.48s\n",
      "iter 6: test acc=0.2959\n",
      "iter 7: train loss/sent=1.2050, time=43.60s\n",
      "iter 7: test acc=0.3109\n",
      "iter 8: train loss/sent=1.0544, time=43.54s\n",
      "iter 8: test acc=0.3181\n",
      "iter 9: train loss/sent=0.9263, time=43.58s\n",
      "iter 9: test acc=0.3136\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "for epoch in range(config.epoch_num):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for words, tag in train:\n",
    "        words = Variable(torch.LongTensor(words)).view(1, -1)\n",
    "        tag = Variable(torch.LongTensor([tag]))\n",
    "        bow.zero_grad()\n",
    "        my_loss = loss_fn(bow(words), tag)\n",
    "        train_loss += my_loss.data[0]\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (epoch, train_loss/len(train), time.time()-start))\n",
    "    \n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for words, tag in dev:\n",
    "        words = Variable(torch.LongTensor(words)).view(1, -1)\n",
    "        scores = bow(words).data.numpy()\n",
    "        predict = np.argmax(scores)\n",
    "        if predict == tag:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: test acc=%.4f\" % (epoch, test_correct/len(dev)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
